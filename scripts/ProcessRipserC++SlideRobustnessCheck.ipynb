{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "strong-investor",
   "metadata": {},
   "source": [
    "### Given sparse matrices representing the time-expanded rips complexes for both car and public transit travel, this file finds candidate regions in a city where new transit hubs should be introduced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "domestic-silicon",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from persim import plot_diagrams,bottleneck\n",
    "import datetime\n",
    "import os\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import shapely\n",
    "import json\n",
    "import networkx as nx\n",
    "import random\n",
    "import requests\n",
    "import polyline\n",
    "import pickle\n",
    "from tqdm.notebook import tqdm,trange\n",
    "import gc\n",
    "import itertools\n",
    "import subprocess\n",
    "\n",
    "from shapely.geometry import MultiPoint, Point, Polygon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "victorian-passport",
   "metadata": {},
   "outputs": [],
   "source": [
    "start = datetime.datetime(2020,2,14,6,5)\n",
    "window = datetime.timedelta(minutes=5)\n",
    "\n",
    "six_am = 1581656400000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "changing-intro",
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph_to_geojson(G,origins,comp_colors=None):\n",
    "    geoJ = {'type':'FeatureCollection'}\n",
    "    features = []\n",
    "    heightScale = 500\n",
    "    if not comp_colors:\n",
    "        clr_choices = [str(i) for i in range(0,10)] + ['A','B','C','D','E','F']\n",
    "        comp_colors = [(comp,'#'+''.join([random.choice(clr_choices) for _ in range(0,6)])) for comp in nx.connected_components(G)]\n",
    "    \n",
    "    \n",
    "    for node in G.nodes:\n",
    "        feat = {'type':'Feature','properties':{}}\n",
    "        \n",
    "        for comp,clr in comp_colors:\n",
    "            if node in comp:\n",
    "                feat['properties']['color'] = clr\n",
    "                break\n",
    "                \n",
    "        node = int(node)\n",
    "        layer = node//len(origins)\n",
    "        loc = node % len(origins)\n",
    "        latlon = origins[loc]\n",
    "        height = layer * heightScale\n",
    "        lon,lat = latlon\n",
    "        lon,lat = float(lon),float(lat)\n",
    "        feat['properties']['lat'] = lat\n",
    "        feat['properties']['lon'] = lon\n",
    "        feat['properties']['height'] = height\n",
    "        \n",
    "                \n",
    "        feat['geometry'] = {'type':'Point','coordinates':[lon,lat]}\n",
    "        features.append(feat)\n",
    "        \n",
    "    for edge in G.edges:\n",
    "        feat = {'type':'Feature','properties':{}}\n",
    "        ori,dest = edge\n",
    "        feat['properties']['weight'] = G.edges[edge]['weight']\n",
    "        \n",
    "        for comp,clr in comp_colors:\n",
    "            if ori in comp:\n",
    "                feat['properties']['color'] = clr\n",
    "                break\n",
    "                \n",
    "        ori = int(ori)\n",
    "        dest = int(dest)\n",
    "        layerori = ori//len(origins)\n",
    "        layerdest = dest//len(origins)\n",
    "        locori = ori % len(origins)\n",
    "        locdest = dest % len(origins)\n",
    "        oriheight = layerori*heightScale\n",
    "        destheight = layerdest*heightScale\n",
    "        \n",
    "        latlonori = origins[locori]\n",
    "        orilon,orilat = latlonori\n",
    "        orilon,orilat = float(orilon),float(orilat)\n",
    "        feat['properties']['orilat'] = orilat\n",
    "        feat['properties']['orilon'] = orilon\n",
    "        feat['properties']['oriheight'] = oriheight\n",
    "        \n",
    "        latlondest = origins[locdest]\n",
    "        destlon,destlat = latlondest\n",
    "        destlon,destlat = float(destlon),float(destlat)\n",
    "        feat['properties']['destlat'] = destlat\n",
    "        feat['properties']['destlon'] = destlon\n",
    "        feat['properties']['destheight'] = destheight\n",
    "        \n",
    "        feat['geometry'] = {'type':'LineString','coordinates':[[orilon,orilat,oriheight],[destlon,destlat,destheight]]}\n",
    "        features.append(feat)\n",
    "    geoJ['features'] = features\n",
    "    return json.dumps(geoJ)\n",
    "\n",
    "\n",
    "\n",
    "def inter_within(inter1,inter2):\n",
    "    return inter2[0] <= inter1[0] <= inter2[1]\n",
    "\n",
    "clist = ['#1f77b4',\n",
    "'#ff7f0e',\n",
    "'#2ca02c',\n",
    "'#d62728',\n",
    "'#9467bd',\n",
    "'#8c564b',\n",
    "'#e377c2',\n",
    "'#7f7f7f',\n",
    "'#bcbd22',\n",
    "'#17becf'\n",
    "]\n",
    "\n",
    "def get_loc_and_time(ind,start_time,window,origins,point_origins):\n",
    "    layer = ind//len(origins)\n",
    "    loc = ind % len(origins)\n",
    "    \n",
    "    t = start_time + window*layer\n",
    "    locs = [x for x in point_origins[loc].coords][0]\n",
    "    return locs,t.strftime(\"%H:%M\")\n",
    "        \n",
    "    \n",
    "def get_loc_and_time_ints(ind,start_time,window,origins,point_origins):\n",
    "    layer = ind//len(origins)\n",
    "    loc = ind % len(origins)\n",
    "    height_scale = 10\n",
    "    t = start_time + window*layer\n",
    "    locs = [x for x in point_origins[loc].coords][0]\n",
    "    return locs,(t.minute + 60* t.hour)*height_scale\n",
    "\n",
    "\n",
    "def get_hull(gen,origins,point_origins):\n",
    "    \n",
    "    edges = [[x%len(origins),y%len(origins)] for x,y in gen if x%len(origins) != y%len(origins)]\n",
    "    map_dicts = {}\n",
    "    #print(edges)\n",
    "    for edge in edges:\n",
    "        src,tar = edge\n",
    "        try:\n",
    "            map_dicts[src] += [src,tar]\n",
    "        except KeyError:\n",
    "            map_dicts[src] = [src,tar]\n",
    "        try:\n",
    "            map_dicts[tar] += [src,tar]\n",
    "        except KeyError:\n",
    "            map_dicts[tar] = [src,tar]\n",
    "    #print(map_dicts)\n",
    "    \n",
    "    cur_node = edges[0][0]\n",
    "    points = [cur_node]\n",
    "    while len(map_dicts) > 0:\n",
    "        next_nodes = map_dicts.pop(cur_node)\n",
    "        valid = [x for x in next_nodes if x != cur_node and x not in points]\n",
    "        \n",
    "        if not valid:\n",
    "            break\n",
    "        next_node = valid[0]\n",
    "        points.append(next_node)\n",
    "        cur_node = next_node\n",
    "        \n",
    "    #print(points)\n",
    "    return Polygon([point_origins[pt] for pt in points] + [point_origins[points[0]]])\n",
    "\n",
    "def cycle_to_geojson(info_list):\n",
    "    to_ret = {'type': 'FeatureCollection', 'features':[]}\n",
    "    for start,end in info_list:\n",
    "        (startx,starty),starth = start\n",
    "        (endx,endy),endh = end\n",
    "        coords = [[startx,starty,starth],[endx,endy,endh]]\n",
    "        feat = {'type':'Feature','properties': {'startLon': startx,\n",
    "                                               'startLat': starty,\n",
    "                                               'startHeight': starth,\n",
    "                                               'endLon': endx,\n",
    "                                               'endLat': endy,\n",
    "                                               'endHeight': endh,\n",
    "                                               \n",
    "                                               'persistence': 0 },\n",
    "               'geometry': {'type': 'LineString',\n",
    "                           'coordinates': coords}}\n",
    "        to_ret['features'].append(feat)\n",
    "    \n",
    "    return to_ret\n",
    "\n",
    "\n",
    "def get_transit_plan(data):\n",
    "    date='2020-02-14'\n",
    "    fromPlace,fromTime,toPlace,toTime = data\n",
    "    r = requests.get('http://localhost:8080/otp/routers/default/plan',params={'fromPlace':fromPlace,\n",
    "                                                                          'toPlace':toPlace,\n",
    "                                                                         'date':date,\n",
    "                                                                         'time':fromTime,\n",
    "                                                                           'mode':'WALK,TRANSIT' })\n",
    "    \n",
    "    js_resp = r.json()\n",
    "    fastest_trip = min([(itin['endTime'],itin) \n",
    "                        for itin in js_resp['plan']['itineraries']],key = lambda x:x[0])\n",
    "    \n",
    "    return fastest_trip[1]\n",
    "\n",
    "def get_transit_plan_car(data):\n",
    "    date='2020-02-14'\n",
    "    fromPlace,fromTime,toPlace,toTime = data\n",
    "    r = requests.get('http://localhost:8080/otp/routers/default/plan',params={'fromPlace':fromPlace,\n",
    "                                                                          'toPlace':toPlace,\n",
    "                                                                         'date':date,\n",
    "                                                                         'time':fromTime,\n",
    "                                                                              'maxPreTransitTime':3000,\n",
    "                                                                             'mode':'WALK,CAR'})\n",
    "    \n",
    "    js_resp = r.json()\n",
    "    \n",
    "    fastest_trip = min([(itin['endTime'],itin) \n",
    "                        for itin in js_resp['plan']['itineraries']],key = lambda x:x[0])\n",
    "    \n",
    "    return fastest_trip[1]\n",
    "\n",
    "def get_transit_plan_bike(data):\n",
    "    date='2020-02-14'\n",
    "    fromPlace,fromTime,toPlace,toTime = data\n",
    "    r = requests.get('http://localhost:8080/otp/routers/default/plan',params={'fromPlace':fromPlace,\n",
    "                                                                          'toPlace':toPlace,\n",
    "                                                                         'date':date,\n",
    "                                                                         'time':fromTime,\n",
    "                                                                             'mode':'WALK,BICYCLE'})\n",
    "    \n",
    "    js_resp = r.json()\n",
    "    fastest_trip = min([(itin['endTime'],itin) \n",
    "                        for itin in js_resp['plan']['itineraries']],key = lambda x:x[0])\n",
    "    \n",
    "    return fastest_trip[1]\n",
    "\n",
    "def legs_to_geojson(pathFeats):\n",
    "    allFeats = []\n",
    "    for pathFeat in pathFeats:\n",
    "        features = []\n",
    "        for leg in pathFeat['legs']:\n",
    "            feat = {}\n",
    "            feat['type'] = 'Feature'\n",
    "            feat['properties']={key:val for key,val in leg.items()}\n",
    "            feat['properties']['globalStartTime'] = pathFeat['startTime']\n",
    "            feat['properties']['globalArrivalTime'] = pathFeat['endTime']\n",
    "            feat['geometry'] = {'type':'LineString', \n",
    "                                'coordinates':[[x[1],x[0]] for x in polyline.decode(leg['legGeometry']['points'])]}\n",
    "            features.append(feat)\n",
    "\n",
    "        collected = {\"type\":\"FeatureCollection\",\"features\":features}\n",
    "        allFeats.append(collected)\n",
    "    return json.dumps(allFeats)\n",
    "\n",
    "def legs_to_geojson_py(pathFeats):\n",
    "    allFeats = []\n",
    "    for pathFeat in pathFeats:\n",
    "        features = []\n",
    "        for leg in pathFeat['legs']:\n",
    "            feat = {}\n",
    "            feat['type'] = 'Feature'\n",
    "            feat['properties']={key:val for key,val in leg.items()}\n",
    "            feat['properties']['globalStartTime'] = pathFeat['startTime']\n",
    "            feat['properties']['globalArrivalTime'] = pathFeat['endTime']\n",
    "            feat['geometry'] = {'type':'LineString', \n",
    "                                'coordinates':[[x[1],x[0]] for x in polyline.decode(leg['legGeometry']['points'])]}\n",
    "            features.append(feat)\n",
    "\n",
    "        collected = {\"type\":\"FeatureCollection\",\"features\":features}\n",
    "        allFeats.append(collected)\n",
    "    return allFeats\n",
    "\n",
    "def legs_to_geojson_edges(pathFeats):\n",
    "    features = []\n",
    "    for pathFeat in pathFeats:\n",
    "        \n",
    "        for leg in pathFeat['legs']:\n",
    "            feat = {}\n",
    "            feat['type'] = 'Feature'\n",
    "            feat['properties']={key:val for key,val in leg.items()}\n",
    "            feat['properties']['globalStartTime'] = pathFeat['startTime']\n",
    "            feat['properties']['globalArrivalTime'] = pathFeat['endTime']\n",
    "            feat['geometry'] = {'type':'LineString', \n",
    "                                'coordinates':[[x[1],x[0]] for x in polyline.decode(leg['legGeometry']['points'])]}\n",
    "            features.append(feat)\n",
    "\n",
    "    collected = {\"type\":\"FeatureCollection\",\"features\":features}\n",
    "        \n",
    "    return collected\n",
    "\n",
    "\n",
    "def legs_to_geojson_with_startTime(pathFeats,startTimes):\n",
    "    allFeats = []\n",
    "    for pathFeat,startTime in zip(pathFeats,startTimes):\n",
    "        features = []\n",
    "        for leg in pathFeat['legs']:\n",
    "            feat = {}\n",
    "            feat['type'] = 'Feature'\n",
    "            feat['properties']={key:val for key,val in leg.items()}\n",
    "            feat['properties']['globalStartTime'] = startTime\n",
    "            feat['properties']['globalArrivalTime'] = pathFeat['endTime']\n",
    "            feat['geometry'] = {'type':'LineString', \n",
    "                                'coordinates':[[x[1],x[0]] for x in polyline.decode(leg['legGeometry']['points'])]}\n",
    "            features.append(feat)\n",
    "\n",
    "        collected = {\"type\":\"FeatureCollection\",\"features\":features}\n",
    "        allFeats.append(collected)\n",
    "    return json.dumps(allFeats)\n",
    "\n",
    "def int_to_time(t):\n",
    "    heightScale = 10\n",
    "    time = t//heightScale\n",
    "    dt = datetime.datetime(2020,2,14,time//60,time%60)\n",
    "    \n",
    "    return dt.strftime('%H:%M') \n",
    "\n",
    "def extract_pts(gens,ind,origins,point_origins):\n",
    "    start_lat = point_origins[gens[ind][0][1][0]%len(origins)].y\n",
    "    start_lon = point_origins[gens[ind][0][1][0]%len(origins)].x\n",
    "    end_lat = point_origins[gens[ind][0][1][1]%len(origins)].y\n",
    "    end_lon = point_origins[gens[ind][0][1][1]%len(origins)].x\n",
    "    \n",
    "    return f'{start_lat},{start_lon}',f'{end_lat},{end_lon}'\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "outdoor-danger",
   "metadata": {},
   "source": [
    "#### The commented out code here computes homology generators using the representative-cycles branch of ripser (which needs to be installed from source after downloading from github here: https://github.com/Ripser/ripser/tree/representative-cycles \n",
    "\n",
    "#### Note that the first argument to subprocess.run should be the location of your built executable for ripser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "great-sending",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[Kreducing column 3896391/3899240 (diameter 100)"
     ]
    }
   ],
   "source": [
    "# for i in range(17,18):\n",
    "i = 4\n",
    "with open(f\"output_slide_hole_{i}.txt\",\"w+\") as f:\n",
    "    p = subprocess.run([\"../../ripser/ripser-representatives\", \"--format\", \"sparse\" ,\"--dim\", \"1\", \"--threshold\", \"100\", f'sparsemat_slide_hole_trans_{i}.txt'], stdout=f)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "framed-conviction",
   "metadata": {},
   "outputs": [],
   "source": [
    "STOCKHOLM_PROJ='EPSG:5850'\n",
    "UNPROJECT='EPSG:4326'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "married-sacrifice",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4cd647e7f60543dcbf51a77e0af58446",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/236315 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/tv/9sk22_ms5x52bzhjvs2z_9ch0000gn/T/ipykernel_92513/2994507482.py:50: DeprecationWarning: Please use `coo_matrix` from the `scipy.sparse` namespace, the `scipy.sparse.coo` namespace is deprecated.\n",
      "  carRips = pickle.load(f)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gen: 11680, Starting birth: 10\n",
      "Gen: 11680, Ending birth: 10\n",
      "Gen: 11682, Starting birth: 10\n",
      "Gen: 11682, Ending birth: 10\n",
      "Gen: 11713, Starting birth: 10\n",
      "Gen: 11713, Ending birth: 10\n",
      "Gen: 11717, Starting birth: 10\n",
      "Gen: 11717, Ending birth: 10\n",
      "Gen: 11737, Starting birth: 10\n",
      "Gen: 11737, Ending birth: 15.0\n",
      "Gen: 11772, Starting birth: 10\n",
      "Gen: 11772, Ending birth: 15.0\n",
      "Gen: 11773, Starting birth: 10\n",
      "Gen: 11773, Ending birth: 20.0\n",
      "Gen: 11775, Starting birth: 10\n",
      "Gen: 11775, Ending birth: 15.0\n",
      "Gen: 11791, Starting birth: 10\n",
      "Gen: 11791, Ending birth: 10\n",
      "Gen: 11804, Starting birth: 10\n",
      "Gen: 11804, Ending birth: 15.0\n",
      "Gen: 11818, Starting birth: 10\n",
      "Gen: 11818, Ending birth: 10\n",
      "Gen: 11819, Starting birth: 10\n",
      "Gen: 11819, Ending birth: 20.0\n",
      "Gen: 11820, Starting birth: 10\n",
      "Gen: 11820, Ending birth: 20.0\n",
      "Gen: 11836, Starting birth: 10\n",
      "Gen: 11836, Ending birth: 15.0\n",
      "Gen: 11837, Starting birth: 10\n",
      "Gen: 11837, Ending birth: 15.0\n",
      "Gen: 11844, Starting birth: 10\n",
      "Gen: 11844, Ending birth: 10\n",
      "Gen: 53259, Starting birth: 15\n",
      "Gen: 53259, Ending birth: 15\n",
      "Gen: 53933, Starting birth: 15\n",
      "Gen: 53933, Ending birth: 15\n",
      "Gen: 54305, Starting birth: 15\n",
      "Gen: 54305, Ending birth: 15\n",
      "Gen: 54556, Starting birth: 15\n",
      "Gen: 54556, Ending birth: 25.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transit_isochrone_dir = 'RobustIsochrones/'\n",
    "car_isochrone_dir = 'SlideIsochronesCar/'\n",
    "# for i in trange(17,18):\n",
    "i = 4\n",
    "subdir = f'Slide_{i}/'\n",
    "graph_file = f'sparsemat_slide_hole_trans_{i}.txt'\n",
    "sample_isochrone = gpd.read_file(transit_isochrone_dir + subdir + f'isochrones_start_21900_end_22200_access_walk_slide_{i}.json')\n",
    "origins = sample_isochrone[sample_isochrone['cutoff'] == 60].loc[:,['fromLon','fromLat']].values\n",
    "point_origins = [shapely.geometry.Point(x,y) for x,y in sample_isochrone[sample_isochrone['cutoff'] == 60].loc[:,['fromLon','fromLat']].values]\n",
    "transit_output_filename = f\"output_slide_hole_{i}.txt\"\n",
    "with open(transit_output_filename,'r') as f:\n",
    "    lines = [line.strip().split(':') \n",
    "        for line in f.readlines()\n",
    "            if line.startswith(' [')]\n",
    "    \n",
    "with open(transit_output_filename,'r') as f:\n",
    "    lines_plus = [line.strip().split(':') \n",
    "                for line in f.readlines()\n",
    "                if line.startswith('+')]\n",
    "    \n",
    "intervals0 = []\n",
    "gens0 = []\n",
    "intervals1 = []\n",
    "gens1 = []\n",
    "for linepair in tqdm(lines):\n",
    "    bd, gen = linepair\n",
    "    birth,death = re.findall(r'\\[(\\d+),(\\d*)',bd)[0]\n",
    "    try:\n",
    "        birth,death = int(birth),int(death)\n",
    "    except ValueError:\n",
    "        # infinite death case\n",
    "        birth = int(birth)\n",
    "        death = np.inf\n",
    "\n",
    "    gen_plus_birth = [x.strip('{} ').split(' ') for x in gen.split(', ')]\n",
    "    if len(gen_plus_birth[0]) > 1:\n",
    "        # dim 1\n",
    "        fixed = [[int(y.strip('[]')) for y in x[0].split(',')] for x in gen_plus_birth]\n",
    "        gens1.append(fixed)\n",
    "        intervals1.append([birth,death])\n",
    "    else:\n",
    "        # dim 0\n",
    "        fixed = [int(x[0][1:-1]) for x in gen_plus_birth]\n",
    "        gens0.append(fixed)\n",
    "        intervals0.append([birth,death])\n",
    "\n",
    "    #print(gen_plus_birth)\n",
    "\n",
    "with open(f'car_coho_slide_{i}.p','rb') as f:\n",
    "    carRips = pickle.load(f)\n",
    "    \n",
    "coc_sets = [[set(x[:-1]) for x in y] for y in carRips['cocycles'][1]]\n",
    "\n",
    "maximpact = np.percentile([np.log(intervals1[i][1] + 1)-np.log(intervals1[i][0] + 1) \n",
    "                            for i,_ in enumerate(intervals1) if intervals1[i][1] < 60],99.9)\n",
    "maxpers = np.percentile([intervals1[i][1] - intervals1[i][0]  for i,_ in enumerate(intervals1) if intervals1[i][1] < 60],99.9)\n",
    "\n",
    "allpers = [i for i,_ in enumerate(intervals1) if intervals1[i][1]  - intervals1[i][0] >= maxpers and intervals1[i][1] < 60]\n",
    "allpers_with_pers = [(i,intervals1[i][1]  - intervals1[i][0]) for i,_ in enumerate(intervals1) if intervals1[i][1]  - intervals1[i][0] >= maxpers and intervals1[i][1] < 60]\n",
    "allimpact_with_endp = [(i,np.log(intervals1[i][1]+1)  - np.log(intervals1[i][0]+1),intervals1[i][0],intervals1[i][1]) for i,_ in enumerate(intervals1) if np.log(intervals1[i][1]+1)  - np.log(intervals1[i][0]+1) >= maximpact and intervals1[i][1] < 60]\n",
    "\n",
    "sorted_allpers_with_pers = sorted(allpers_with_pers,key=lambda x: x[1])\n",
    "sorted_allimpact = sorted(allimpact_with_endp,key=lambda x:x[1])\n",
    "mode_imb_allpers = []\n",
    "modified_gens = []\n",
    "\"\"\"for tmpind,(ind,pers) in enumerate(sorted_allpers_with_pers[-10:]):\n",
    "    testgen = [set(x) for x in gens1[ind]]\n",
    "    relevant_coc = [(coc,carRips['dgms'][1][i][1]) for i,coc in enumerate(coc_sets) if inter_within(intervals1[ind],\n",
    "                                                                        carRips['dgms'][1][i]) ]\n",
    "    is_zero = True\n",
    "    cur_birth = intervals1[ind][0]\n",
    "    print(f\"Gen: {ind}, Starting birth: {cur_birth}\")\n",
    "    for check,death in relevant_coc:\n",
    "        cursum = 0\n",
    "        for edge in check:\n",
    "            for gen_edge in testgen:\n",
    "                if gen_edge.difference(edge):\n",
    "                    cursum += 1\n",
    "        if cursum % 2 != 0:\n",
    "            if cur_birth < death:\n",
    "                cur_birth = death\n",
    "    print(f\"Gen: {ind}, Ending birth: {cur_birth}\")\n",
    "    if cur_birth == intervals1[ind][0]:\n",
    "        mode_imb_allpers.append((ind,death-cur_birth))\n",
    "    else:\n",
    "        modified_gens.append((ind,death-cur_birth))\"\"\"\n",
    "\n",
    "mode_imb_allimpact = []\n",
    "modified_gens_impact = []\n",
    "for tmpind,(ind,imp,birth,orig_death) in enumerate(sorted_allimpact[-50:-30]):\n",
    "    testgen = [set(x) for x in gens1[ind]]\n",
    "    relevant_coc = [(coc,carRips['dgms'][1][i][1]) for i,coc in enumerate(coc_sets) if inter_within(intervals1[ind],\n",
    "                                                                        carRips['dgms'][1][i]) ]\n",
    "    is_zero = True\n",
    "    cur_birth = birth\n",
    "    print(f\"Gen: {ind}, Starting birth: {cur_birth}\")\n",
    "    for check,death in relevant_coc:\n",
    "        cursum = 0\n",
    "        for edge in check:\n",
    "            for gen_edge in testgen:\n",
    "                if gen_edge.difference(edge):\n",
    "                    cursum += 1\n",
    "        if cursum % 2 != 0:\n",
    "            if cur_birth < death:\n",
    "                cur_birth = death\n",
    "    print(f\"Gen: {ind}, Ending birth: {cur_birth}\")\n",
    "    if cur_birth == intervals1[ind][0]:\n",
    "        mode_imb_allimpact.append((ind,imp,birth,orig_death))\n",
    "    else:\n",
    "        modified_gens_impact.append((ind,np.log(death+1)-np.log(cur_birth+1),cur_birth,orig_death))\n",
    "    \n",
    "for j,gentup in enumerate(mode_imb_allpers):\n",
    "    max_gen = gentup[0]\n",
    "    max_feat_pers_int = sorted([[get_loc_and_time_ints(x,start,window,origins,point_origins),get_loc_and_time_ints(y,start,window,origins,point_origins)] for x,y in gens1[max_gen]],key=lambda x: x[0][1])\n",
    "    with open(f\"KeplerViz/CheckFeatSlide_{i}_gen_{j}.json\",\"w+\") as f:\n",
    "        f.write(json.dumps(cycle_to_geojson(max_feat_pers_int)))\n",
    "        \n",
    "\n",
    "#\n",
    "        \n",
    "for j,gentup in enumerate(mode_imb_allpers):\n",
    "    max_gen = gentup[0]\n",
    "    gen = gens1[max_gen]\n",
    "    feat = shapely.geometry.mapping(get_hull(gen,origins,point_origins))\n",
    "    geoj_dict = {'type':'FeatureCollection','features':[{'type':'Feature',\n",
    "                                                            'geometry':feat}]}\n",
    "                                    \n",
    "    with open(f\"KeplerViz/CheckPolySlide_{i}_gen_{j}.json\",\"w+\") as f:\n",
    "        f.write(json.dumps(geoj_dict))\n",
    "                                    \n",
    "for j,gentup in enumerate(modified_gens):\n",
    "    max_gen = gentup[0]\n",
    "    max_feat_pers_int = sorted([[get_loc_and_time_ints(x,start,window,origins,point_origins),get_loc_and_time_ints(y,start,window,origins,point_origins)] for x,y in gens1[max_gen]],key=lambda x: x[0][1])\n",
    "    with open(f\"KeplerViz/CheckModFeatSlide_{i}_gen_{j}.json\",\"w+\") as f:\n",
    "        f.write(json.dumps(cycle_to_geojson(max_feat_pers_int)))\n",
    "        \n",
    "for j,gentup in enumerate(modified_gens):\n",
    "    max_gen = gentup[0]\n",
    "    gen = gens1[max_gen]\n",
    "    feat = shapely.geometry.mapping(get_hull(gen,origins,point_origins))\n",
    "    geoj_dict = {'type':'FeatureCollection','features':[{'type':'Feature',\n",
    "                                                            'geometry':feat}]}\n",
    "                                    \n",
    "    with open(f\"KeplerViz/CheckModPolySlide_{i}_gen_{j}.json\",\"w+\") as f:\n",
    "        f.write(json.dumps(geoj_dict))\n",
    "                                    \n",
    "for j,gentup in enumerate(mode_imb_allimpact):\n",
    "    max_gen = gentup[0]\n",
    "    max_feat_pers_int = sorted([[get_loc_and_time_ints(x,start,window,origins,point_origins),get_loc_and_time_ints(y,start,window,origins,point_origins)] for x,y in gens1[max_gen]],key=lambda x: x[0][1])\n",
    "    with open(f\"KeplerViz/CheckImpactFeatSlide_{i}_gen_{j}.json\",\"w+\") as f:\n",
    "        f.write(json.dumps(cycle_to_geojson(max_feat_pers_int)))\n",
    "                                    \n",
    "    \n",
    "        \n",
    "for j,gentup in enumerate(mode_imb_allimpact):\n",
    "    max_gen = gentup[0]\n",
    "    gen = gens1[max_gen]\n",
    "    feat = shapely.geometry.mapping(get_hull(gen,origins,point_origins))\n",
    "    geoj_dict = {'type':'FeatureCollection','features':[{'type':'Feature',\n",
    "                                                            'geometry':feat}]}\n",
    "    with open(f\"KeplerViz/CheckImpactPolySlide_{i}_gen_{j}.json\",\"w+\") as f:\n",
    "        f.write(json.dumps(geoj_dict))\n",
    "                                    \n",
    "for j,gentup in enumerate(modified_gens_impact):\n",
    "    max_gen = gentup[0]\n",
    "    max_feat_pers_int = sorted([[get_loc_and_time_ints(x,start,window,origins,point_origins),get_loc_and_time_ints(y,start,window,origins,point_origins)] for x,y in gens1[max_gen]],key=lambda x: x[0][1])\n",
    "    with open(f\"KeplerViz/CheckModImpactFeatSlide_{i}_gen_{j}.json\",\"w+\") as f:\n",
    "        f.write(json.dumps(cycle_to_geojson(max_feat_pers_int)))\n",
    "        \n",
    "for j,gentup in enumerate(modified_gens_impact):\n",
    "    max_gen = gentup[0]\n",
    "    gen = gens1[max_gen]\n",
    "    feat = shapely.geometry.mapping(get_hull(gen,origins,point_origins))\n",
    "    geoj_dict = {'type':'FeatureCollection','features':[{'type':'Feature',\n",
    "                                                            'geometry':feat}]}\n",
    "                                    \n",
    "    with open(f\"KeplerViz/CheckModImpactPolySlide_{i}_gen_{j}.json\",\"w+\") as f:\n",
    "        f.write(json.dumps(geoj_dict))\n",
    "    \n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "legislative-rover",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(242410, 0.6613984822453651, 15, 30),\n",
       " (406157, 0.6690496289808849, 20, 40),\n",
       " (359863, 0.8109302162163288, 15, 35),\n",
       " (74531, 0.8602012652231115, 10, 25),\n",
       " (74663, 0.8602012652231115, 10, 25)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mode_imb_allimpact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cordless-grace",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.6613984822453651,\n",
       " 0.6690496289808849,\n",
       " 0.8109302162163288,\n",
       " 0.8602012652231115,\n",
       " 0.8602012652231115]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[np.log(tup[3]+1) - np.log(tup[2]+1) for tup in mode_imb_allimpact]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "infectious-parker",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(242403, 0.6613984822453651, 15, 30),\n",
       " (242405, 0.6613984822453651, 15, 30),\n",
       " (242410, 0.6613984822453651, 15, 30),\n",
       " (406157, 0.6690496289808849, 20, 40),\n",
       " (359532, 0.8109302162163288, 15, 35),\n",
       " (359863, 0.8109302162163288, 15, 35),\n",
       " (74502, 0.8602012652231115, 10, 25),\n",
       " (74531, 0.8602012652231115, 10, 25),\n",
       " (74663, 0.8602012652231115, 10, 25),\n",
       " (74683, 0.8602012652231115, 10, 25),\n",
       " (74849, 0.8602012652231115, 10, 25),\n",
       " (74934, 0.8602012652231115, 10, 25),\n",
       " (75082, 0.8602012652231115, 10, 25),\n",
       " (75489, 0.8602012652231115, 10, 25),\n",
       " (75893, 0.8602012652231115, 10, 25),\n",
       " (75896, 0.8602012652231115, 10, 25),\n",
       " (75973, 0.8602012652231115, 10, 25),\n",
       " (76259, 0.8602012652231115, 10, 25),\n",
       " (76579, 0.8602012652231115, 10, 25),\n",
       " (76595, 0.8602012652231115, 10, 25),\n",
       " (76712, 0.8602012652231115, 10, 25),\n",
       " (76931, 0.8602012652231115, 10, 25),\n",
       " (77058, 0.8602012652231115, 10, 25),\n",
       " (77532, 0.8602012652231115, 10, 25),\n",
       " (77733, 0.8602012652231115, 10, 25),\n",
       " (77740, 0.8602012652231115, 10, 25),\n",
       " (77886, 0.8602012652231115, 10, 25),\n",
       " (77975, 0.8602012652231115, 10, 25),\n",
       " (78041, 0.8602012652231115, 10, 25),\n",
       " (78245, 0.8602012652231115, 10, 25),\n",
       " (78634, 0.8602012652231115, 10, 25),\n",
       " (79004, 0.8602012652231115, 10, 25),\n",
       " (79339, 0.8602012652231115, 10, 25),\n",
       " (79509, 0.8602012652231115, 10, 25),\n",
       " (79628, 0.8602012652231115, 10, 25),\n",
       " (80098, 0.8602012652231115, 10, 25),\n",
       " (81118, 0.8602012652231115, 10, 25),\n",
       " (81736, 0.8602012652231115, 10, 25),\n",
       " (84885, 0.8602012652231115, 10, 25),\n",
       " (86651, 0.8602012652231115, 10, 25),\n",
       " (92173, 0.8602012652231115, 10, 25),\n",
       " (107922, 0.8602012652231115, 10, 25),\n",
       " (123744, 0.8602012652231115, 10, 25),\n",
       " (128799, 0.8602012652231115, 10, 25),\n",
       " (134773, 0.8602012652231115, 10, 25),\n",
       " (387986, 0.9409833444645268, 15, 40),\n",
       " (151678, 1.0360919316867756, 10, 30),\n",
       " (152069, 1.0360919316867756, 10, 30),\n",
       " (153977, 1.0360919316867756, 10, 30),\n",
       " (158685, 1.0360919316867756, 10, 30)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_allimpact[-50:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "limiting-athletics",
   "metadata": {},
   "source": [
    "### if mode_imb_allimpact is empty, search again for highest impact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "absolute-bangladesh",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.7 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "6d73269661cd96e104e0c8367fe54e880a091a1681a7536cc663a21382d9b9ea"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
